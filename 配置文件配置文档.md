GLMLS_LG 模型配置文档


配置文件结构

`config.json`是 GLMLS_LG 模型的配置文件，用于定义模型的架构参数和训练参数。以下是详细的配置说明：


```json
{
    "model": {
        "block_size": 128,
        "n_layer": 6,
        "n_head": 4,
        "n_embed": 384,
        "vocab_size": 50304,
        "dropout": 0.1
    },
    "training": {
        "mode": "resume",
        "batch_size": 32,
        "learning_rate": 0.0001,
        "num_epochs": 10,
        "log_interval": 100,
        "checkpoint_dir": "./checkpoints",
        "data_path": "./training_data.txt"
    }
}
```



详细参数说明


模型参数(`model`)


参数名称	类型	默认值	描述	
block_size	int	128	模型处理的序列块大小，决定了模型一次处理的 token 数量。	
n_layer	int	6	模型中 Transformer 的层数。	
n_head	int	4	每个 Transformer 层中的注意力头数。	
n_embed	int	384	每个 token 的嵌入维度大小。	
vocab_size	int	50304	词汇表的大小，即模型可以处理的不同 token 的数量。	
dropout	float	0.1	Dropout 概率，用于防止过拟合。	




训练参数(`training`)


参数名称	类型	默认值	描述	
mode	string	"resume"	训练模式，可选值为`"resume"`（从检查点继续训练）或`"scratch"`（从头开始训练）。	
batch_size	int	32	每个训练批次的样本数量。	
learning_rate	float	0.0001	学习率，控制优化器更新模型参数的速度。	
num_epochs	int	10	训练的总轮数，即模型遍历整个训练数据集的次数。	
log_interval	int	100	每隔多少个批次打印一次训练日志。	
checkpoint_dir	string	"./checkpoints"	保存模型检查点的目录路径。	
data_path	string	"./training_data.txt"	训练数据文件的路径。	




配置文件示例

以下是一个完整的`config.json`示例文件：


```json
{
    "model": {
        "block_size": 128,
        "n_layer": 6,
        "n_head": 4,
        "n_embed": 384,
        "vocab_size": 50304,
        "dropout": 0.1
    },
    "training": {
        "mode": "resume",
        "batch_size": 32,
        "learning_rate": 0.0001,
        "num_epochs": 10,
        "log_interval": 100,
        "checkpoint_dir": "./checkpoints",
        "data_path": "./training_data.txt"
    }
}
```



配置说明


1. 模型参数：

• `block_size`：决定了模型处理的序列长度。较大的值可以使模型捕捉更长的上下文，但也会增加内存消耗。

• `n_layer`：Transformer 的层数越多，模型的表达能力越强，但训练和推理时间也会增加。

• `n_head`：注意力头数决定了模型在每个层中并行计算注意力的次数。

• `n_embed`：嵌入维度越大，模型可以捕捉更复杂的特征，但也会增加计算量。

• `vocab_size`：词汇表大小应与训练数据中的唯一 token 数量匹配。

• `dropout`：Dropout 是一种正则化技术，可以防止模型过拟合。


2. 训练参数：

• `mode`：选择训练模式。`"resume"`表示从上次保存的检查点继续训练，`"scratch"`表示从头开始训练。

• `batch_size`：较大的批次大小可以提高训练效率，但需要更多的内存。

• `learning_rate`：学习率决定了模型参数更新的步长。过大的学习率可能导致训练不稳定，过小的学习率会减慢训练速度。

• `num_epochs`：训练的总轮数越多，模型可能越准确，但也可能过拟合。

• `log_interval`：控制训练日志的打印频率。

• `checkpoint_dir`：指定保存模型检查点的目录。

• `data_path`：指定训练数据文件的路径。

通过合理调整这些参数，可以优化模型的性能和训练效率。